{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import re, os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(input_path, output_path):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "        - input_path: Input data (article) directory path\n",
    "        - output_path: Path for printing the processed data.   \n",
    "\n",
    "    This function preprocesses the input file (provided artice) in the following manner:\n",
    "        - Lowercases the data\n",
    "        - Remove any special characters/HTML tags\n",
    "        - removes new line seperation\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join(word_tokenize(text.translate(str.maketrans('', '', string.punctuation))))  # Remove new lines \n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating embeddings and storing in local database using FAISS\n",
    "def database_embeddings(data_path, chunk_size, chunk_overlap, save_local):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "        - data_path: Directory consisting of preprocessed input file (can be multiple)\n",
    "        - chunk_size: chunk size of text fed into the embeddings model\n",
    "        - chunk_overlap: Amount of chunk overlap\n",
    "        - save_local: boolean variable for storing FAISS database localy\n",
    "    output:\n",
    "        - database containing word embeddings\n",
    "    \n",
    "        This function utilizes hugging face embeddings model through langchain framework,\n",
    "        the most popular and efficient model was selected.\n",
    "        FAISS is used to store the embeddings into a local database.\n",
    "    \"\"\"\n",
    "    # define which documents to load\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "    # splitting into chunks based on defined variable\n",
    "    document = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=chunk_overlap)\n",
    "    texts = splitter.split_documents(document)\n",
    "    # Alternative approach would be too use transformers library with auto tokenizer from a pretrained model (same model as below can be used)\n",
    "    # Generating embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    # create and save the local database using FAISS\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    # To store database (If needed)\n",
    "    if save_local:\n",
    "        db.save_local(\"faiss\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating pre-trained LLM to answer users questions based on local content\n",
    "def QA_model(db, model,max_tokens, temperature):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "        - db: Database containing word embeddings\n",
    "        - model: Option to choose a large language model from different LLAMA 2 variants. (other models can also be used),\n",
    "                  currently three available locally in 'Models' directory:\n",
    "                    llama-2-7b-chat.ggmlv3.q8_0 (latest and best performance) , llama-2-7b-chat.ggmlv3.q6_K       llama-2-7b-chat.ggmlv3.q3_K_M\n",
    "        - max_tokens: Max number of tokens the model generates in its output\n",
    "        - temperature: Randomness of the output. (0.0 is the min and 1.0 is the max)\n",
    "    output:\n",
    "    A LLM model oriented for answering questions\n",
    "    \n",
    "    \"\"\"\n",
    "   # load the language model\n",
    "    llm = CTransformers(model='Models/{}.bin'.format(model),\n",
    "                        model_type='llama',\n",
    "                        config={'max_new_tokens': max_tokens, 'temperature': temperature,'repetition_penalty': 1.1})\n",
    "    # repetition_penalty = without this output begins repeating \n",
    "    # Creating template to prompt the model\n",
    "    template = \"\"\"Please use solely the given information to answer user's prompt. Do not provide any external answers.\"\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer with respect to the given context.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    # prepare a version of the llm pre-loaded with the local content\n",
    "    retriever = db.as_retriever(search_kwargs={'k': 2})     # k defines how many documents are returned\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=['context', 'question'])\n",
    "    qa_llm = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                        chain_type='stuff',        # Best for small documents, most straightforward of the document chains.\n",
    "                                        # It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\n",
    "                                        # This chain is well-suited for applications where documents are small and only a few are passed in for most calls.\n",
    "                                        retriever=retriever,\n",
    "                                        return_source_documents=True,\n",
    "                                        chain_type_kwargs={'prompt': prompt})\n",
    "    return qa_llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting model\n",
    "def QA(llm, questions):\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "            - llm: Trained large language model (LLAMA 2)\n",
    "            - questions: A list of prompts/queries from the user for which the model gives the answer. (Can be 1 or many)\n",
    "    \n",
    "    This function prompts the LLM to answer user defined questions which it prints and stores in a .txt file. \n",
    "    \"\"\"\n",
    "    # Generating evaluation \n",
    "    if os.path.isdir('model_results') == False:\n",
    "        os.mkdir('model_results')\n",
    "    for question in questions:\n",
    "        output = llm({'query': question})\n",
    "        print(\"Question:\\n\",output[\"query\"],'\\n')\n",
    "        print(\"Answer:\\n\",output[\"result\"],'\\n')\n",
    "        print(\"Source:\\n\",output[\"source_documents\"][0],'\\n')\n",
    "        print(\"*----------------------------------------------------------------------------*\\n\\n\")\n",
    "        with open('model_results/output.txt', 'a') as f:\n",
    "            f.write(\"Question:\\n{}\\n\".format(output[\"query\"]))\n",
    "            f.write(\"Answer:\\n{}\\n\".format(output[\"result\"]))\n",
    "            f.write(\"Source:\\n{}\\n\".format(output[\"source_documents\"][0]))\n",
    "            f.write(\"*----------------------------------------------------------------------------*\\n\\n\")\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "# Data import and export\n",
    "input_file_path = 'raw_data/data.txt'  # Unprocessed input data\n",
    "output_file_path = 'preprocessed_data/processed_data.txt'  # processed data\n",
    "# Apply preprocessing\n",
    "preprocess_text(input_file_path, output_file_path)\n",
    "# Generating embeddings and storing in local database using FAISS\n",
    "db = database_embeddings(\"preprocessed_data\", chunk_size=500, chunk_overlap=50, save_local=False)\n",
    "# Generating LLM oriented for answering users questions based on local database\n",
    "qa_llm = QA_model(db,\"llama-2-7b-chat.ggmlv3.q8_0\", max_tokens=500, temperature=0.01)\n",
    "# Questions (can be 1 or many)\n",
    "questions = [\"Which teams played in the NFL Kickoff Game to begin the 2022 season, and what was the result?\", \n",
    "      \"Why was the game between Buffalo and Cincinnati cancelled?\",\n",
    "      \"What new name was adopted by the former Washington Redskins for the 2022 NFL season?\",\n",
    "      \"State the career trajectory of Tra Blake leading up to his promotion to replace Tony Corrente as a referee in the 2022 NFL season.\",\n",
    "      \"Who was hired as the league's first Asian-American NFL official and from which conference was this individual recruited?\",\n",
    "      \"Who was hired as the league's first Asian-American NFL official and from which conference was this individual recruited?\",\n",
    "      \"In regard to the inclusive hiring training announced at the NFL Fall League Meeting on October 18, who is mandated to participate and under what circumstances must this training be undertaken?\"]\n",
    "# Prompting model\n",
    "QA(qa_llm,questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
