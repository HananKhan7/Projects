
# Personalized Toxicity Detection using Mixtral 7 and Llama 3

This repository contains two Python scripts that implement personalized toxicity detection using the Mixtral 7B and Llama 3 models. These scripts are part of a project aimed at enhancing the accuracy and relevance of toxicity detection by incorporating user profiles of annotators to provide more personalized outputs.

## Project Description

In this project, we aim to improve the detection of toxic comments by personalizing the model's responses based on the profiles of the annotators. This approach leverages the strengths of both Mixtral 7B and Llama 3 models to provide a nuanced understanding of what constitutes toxicity for different individuals.

### Objectives

- **Personalized Toxicity Detection**: Utilize user profiles to tailor the toxicity detection model's output, making it more relevant and accurate.
- **Model Implementation**: Implement Mixtral 7B and Llama 3 models for toxicity detection.
- **Profile Integration**: Feed user profiles into the models to obtain personalized toxicity ratings.

## Files in the Repository

1. **Personalized_toxicity_classification_Mixtral_7B.py**: This script utilizes the Mixtral 7B model for personalized toxicity detection.
2. **Personalized_toxicity_classification_LLAMA3_8B.py**: This script leverages the Llama 3 model for personalized toxicity detection.

## Steps Taken

### Data Preparation

1. **Load User Profiles**: Annotator profiles are loaded and preprocessed to feed into the models.
2. **Prepare Training Data**: Training data is prepared by combining comments with annotator profiles to create a comprehensive dataset.

### Model Implementation

1. **Load Models**: Mixtral 7B and Llama 3 models are loaded using the Huggingface library.
2. **Generate Prompts**: Prompts are generated by integrating user profiles with the text comments to be analyzed.

### Prediction and Evaluation

1. **Predict Toxicity**: The models predict toxicity scores based on the provided prompts.
2. **Evaluate Predictions**: The predicted scores are compared with the actual toxicity scores to evaluate the model's performance.

## How to Use

### Prerequisites

- Python 3.8 or higher
- Huggingface Transformers library
- PyTorch

### Installation

1. Install the required libraries:

### Running the Scripts

1. **Personalized_toxicity_classification_Mixtral_7B.py**:
   ```bash
   python Personalized_toxicity_classification_Mixtral_7B.py
   ```

2. **Personalized_toxicity_classification_LLAMA3_8B.py**:
   ```bash
   python Personalized_toxicity_classification_LLAMA3_8B.py
   ```

## Results

The output will be a JSON file containing the toxicity ratings for each comment, personalized based on the user profiles of the annotators. The results can be used to analyze the effectiveness of personalized toxicity detection and to further refine the models.

